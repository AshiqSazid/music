# Music Intelligence Assistant - Full Streamlit App with Ollama + Mistral LLM Integration

import streamlit as st
import numpy as np
import pandas as pd
import torchaudio
from torchaudio.models import yamnet
from torchaudio.pipelines import YAMNET
import librosa
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import openl3
import tempfile
import json
import matplotlib.pyplot as plt
from textblob import TextBlob
import re
import requests

# === Streamlit Setup ===
st.set_page_config(page_title="Music Insight AI", layout="wide")
st.title("\U0001F3B5 Music Intelligence Assistant")

st.markdown("""
Analyze your song with research-backed AI:
- Detect instruments and their proportion across time
- Predict virality using real audio features
- Generate LLM prompts for marketing, playlisting, and track improvement
- Analyze lyric sentiment, extract dominant themes
- Check alignment between audio mood and lyrical message
- Get instant feedback from a local LLM using Ollama + Mistral
""")

uploaded_file = st.file_uploader("Upload a .wav audio file", type=["wav"])
upload_lyrics = st.file_uploader("(Optional) Upload lyrics as .txt file", type=["txt"])

lyric_sentiment = ""
theme_keywords = []
audio_lyric_similarity = ""
raw_lyrics = ""

# === Ollama Query Function ===
def query_ollama_mistral(prompt, model="mistral"):
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model, "prompt": prompt, "stream": False}
        )
        return response.json().get("response", "No response returned from Mistral.")
    except Exception as e:
        return f"Error querying Mistral via Ollama: {e}"

if upload_lyrics:
    raw_lyrics = upload_lyrics.read().decode('utf-8')
    blob = TextBlob(raw_lyrics)
    sentiment_score = blob.sentiment.polarity
    sentiment_label = "Positive" if sentiment_score > 0.2 else "Negative" if sentiment_score < -0.2 else "Neutral"
    lyric_sentiment = f"{sentiment_label} ({sentiment_score:.2f})"
    words = re.findall(r'\b\w{4,}\b', raw_lyrics.lower())
    stopwords = set(['this', 'that', 'with', 'your', 'from', 'there', 'when', 'will', 'what', 'have', 'just', 'like'])
    filtered = [w for w in words if w not in stopwords]
    theme_counts = pd.Series(filtered).value_counts().head(5)
    theme_keywords = list(theme_counts.index)

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    waveform, sr = torchaudio.load(tmp_path)
    waveform = waveform.mean(dim=0).unsqueeze(0)
    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
    waveform_resampled = resampler(waveform)

    # === Instrument Detection ===
    st.subheader("1. Instrument Detection and Proportion Estimation")
    model = yamnet(pretrained=True).eval()
    labels = YAMNET_CLASS_MAP

    with torch.inference_mode():
        scores, _, _ = model(waveform_resampled)
    scores_np = scores.numpy()
    time_steps = scores_np.shape[0]
    mean_scores = scores_np.mean(axis=0)

    instrument_keywords = ['drum', 'guitar', 'piano', 'violin', 'vocal', 'saxophone', 'flute', 'bass', 'synth', 'trumpet']
    instrument_scores = {}
    for i, label in enumerate(labels):
        for keyword in instrument_keywords:
            if keyword in label.lower():
                instrument_scores[keyword] = instrument_scores.get(keyword, 0) + mean_scores[i]

    total_score = sum(instrument_scores.values())
    proportions = {k.capitalize(): round(v / total_score * 100, 2) for k, v in instrument_scores.items()} if total_score > 0 else {}

    if proportions:
        st.write("### Estimated Instrument Proportions:")
        st.bar_chart(pd.Series(proportions))

        st.write("### Temporal Dynamics of Instrument Detection")
        time_axis = np.arange(time_steps)
        fig, ax = plt.subplots()
        for keyword in instrument_keywords:
            idxs = [i for i, label in enumerate(labels) if keyword in label.lower()]
            if idxs:
                segment = scores_np[:, idxs].sum(axis=1)
                ax.plot(time_axis, segment, label=keyword)
        ax.set_xlabel("Time Segments")
        ax.set_ylabel("Confidence")
        ax.set_title("Instrument Confidence Over Time")
        ax.legend()
        st.pyplot(fig)
    else:
        st.warning("No recognizable instruments detected.")

    # === Virality Prediction ===
    st.subheader("2. Virality Prediction from Audio Features")
    audio_np = waveform.squeeze().numpy()
    tempo, _ = librosa.beat.beat_track(y=audio_np, sr=sr)
    rms = np.mean(librosa.feature.rms(y=audio_np))
    spec_cent = np.mean(librosa.feature.spectral_centroid(y=audio_np, sr=sr))
    mfccs = librosa.feature.mfcc(y=audio_np, sr=sr, n_mfcc=13)
    mfcc_1 = np.mean(mfccs[0])
    mfcc_2 = np.mean(mfccs[1])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio_np))
    chroma = librosa.feature.chroma_stft(y=audio_np, sr=sr)
    chroma_stdev = np.std(chroma)

    features = pd.DataFrame([{
        "tempo": tempo,
        "rms_energy": rms,
        "spectral_centroid": spec_cent,
        "mfcc_1": mfcc_1,
        "mfcc_2": mfcc_2,
        "zero_crossing_rate": zcr,
        "chroma_stdev": chroma_stdev
    }])

    np.random.seed(42)
    train_X = pd.DataFrame({
        "tempo": np.random.normal(120, 10, 200),
        "rms_energy": np.random.uniform(0.1, 0.9, 200),
        "spectral_centroid": np.random.uniform(1000, 4000, 200),
        "mfcc_1": np.random.normal(-200, 20, 200),
        "mfcc_2": np.random.normal(0, 10, 200),
        "zero_crossing_rate": np.random.uniform(0.01, 0.2, 200),
        "chroma_stdev": np.random.uniform(0.01, 0.1, 200)
    })
    train_y = np.clip((
        0.3 * train_X["tempo"] +
        200 * train_X["rms_energy"] +
        0.01 * train_X["spectral_centroid"] +
        0.05 * train_X["mfcc_1"] +
        0.1 * train_X["mfcc_2"] -
        100 * train_X["zero_crossing_rate"] +
        300 * train_X["chroma_stdev"] +
        np.random.normal(0, 5, 200)
    ) / 400 * 100, 0, 100)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(train_X)
    model_rf = RandomForestRegressor(n_estimators=100, random_state=42)
    model_rf.fit(X_scaled, train_y)
    virality_score = model_rf.predict(scaler.transform(features))[0]

    st.metric("Predicted Virality Score", f"{virality_score:.2f}/100")

    # === LLM Prompt + Ollama Call ===
    st.subheader("3. LLM-Based Decision Insights (via Mistral)")
    prompt_data = {
        "instruments": proportions,
        "virality_score": round(virality_score, 2),
        "audio_features": features.to_dict(orient="records")[0],
        "lyric_sentiment": lyric_sentiment,
        "lyric_themes": theme_keywords,
        "lyric_audio_similarity": audio_lyric_similarity,
        "lyrics_excerpt": raw_lyrics[:300] + ("..." if len(raw_lyrics) > 300 else "")
    }

    deepseek_prompt = f"""
You are a music strategist and creative AI. Analyze this:

{json.dumps(prompt_data, indent=2)}

Questions:
1. Would this song fit workout, chill, romantic, or viral TikTok playlists? Why?
2. What production, lyrical, or structure improvements would boost its impact?
3. Suggest marketing strategy and distribution focus (Spotify, Reels, shortform, etc).
"""

    st.code(deepseek_prompt, language="markdown")

    if st.button("Get AI Feedback from Mistral (via Ollama)"):
        with st.spinner("Querying Mistral..."):
            ai_response = query_ollama_mistral(deepseek_prompt)
        st.markdown("### ðŸ”® AI Feedback")
        st.markdown(ai_response)
else:
    st.info("Please upload a .wav audio file to begin analysis.")